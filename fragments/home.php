<div>
	<p>ProseVis is a visualization tool developed as part of a use case
		supported by the Andrew W. Mellon Foundation through a grant titled
		"SEASR Services," in which we seek to identify other features than the
		"word" to analyze texts. These features comprise sound including
		parts-of-speech, accent, phoneme, stress, tone, break index.</p>
</div>

<div>
	<p>
		ProseVis allows a reader to map the features extracted from OpenMary (<a
			href="http://mary.dfki.de/" title="OpenMary Link">http://mary.dfki.de/</a>)
		Text-to-speech System and predictive classification data to the
		"original" text. We developed this project with the ultimate goal of
		facilitating a reader's ability to analyze and disseminate the results
		in human readable form. Research has shown that mapping the data to
		the text in its original form allows for the kind of human reading
		that literary scholars engage: words in the context of phrases,
		sentences, lines, stanzas, and paragraphs (Clement 2008). Recreating
		the context of the page not only allows for the simultaneous
		consideration of multiple representations of knowledge or readings
		(since every reader's perspective on the context will be different)
		but it also allows for a more transparent view of the underlying data.
		If a human can see the data (the syllables, the sounds, the
		parts-of-speech) within the context in which they are used to reading,
		with the data mapped back onto the full text, then the reader is
		empowered within this familiar context to read what might otherwise be
		an unfamiliar representation tabular representation of the text. For
		these reasons, we developed ProseVis as a reader interface to allow
		scholars to work with the data in a language or context in which we
		are used to saying things about the world.
	</p>
	<p>
		<strong>Publications</strong>
	</p>
	<p>
		Clement, T., Tcheng, D., Auvil, L., Capitanu, B., and Barbosa, J.
		"Distant Listening to Gertrude Stein's 'Melanctha': Using Similarity
		Analysis in a Discovery Paradigm to Analyze Prosody and Author
		Influence." <i>Literary and Linguistic Computing</i> (accepted for
		publication).
	</p>
	<p>
		Clement, T. "Distant Listening or Playing Visualizations Pleasantly
		with the Eyes and Ears." <i>Digital Studies / Le champ
			num&eacute;rique</i>. 3.2 (2012). Online May 2013.
	</p>
	<p>
		Clement, T., Tcheng, D., Auvil, L., Capitanu, B. Monroe, M. "Sounding
		for Meaning: Using Theories of Knowledge Representation to Analyze
		Aural Patterns in Texts." <i>digital humanities quarterly</i>. 7.1
		(2013). Online May 2013.
	</p>
	<p>
		Clement, T. "Methodologies in the Digital Humanities for Analyzing
		Aural Patterns in Texts." <i>Proceedings of the 2012 iConference</i>.
		New York, NY, USA: ACM, 2012. 287�293. Web. 21 Feb. 2012.
	
	
	<p>
		<strong>Talks</strong>
	</p>
	<p>"Sounding it Out: Modeling Aurality for Large-Scale Text Collection
		Analysis." Digital Humanities Conference, University of Hamburg,
		Germany (July 2012).</p>
	<p>Clement, T. "Sounding it Out: Modeling Aurality for Large-Scale Text
		Collection Analysis." Big Data: Text Mining in the Digital Humanities,
		McGill University (May 2012).
	
	
	<p>Clement, T. "Sounding for Meaning: Knowledge Representation and
		Methodologies in the Digital Humanities for Analyzing Aural Textual
		Patterns." Critical Digital Humanities Research Group, UC Riverside
		(April 2012).</p>
	<p>"Circles: Sounding Stein�s Texts by Using Digital Tools for Distant
		Listening." Modern Language Association (MLA) Conference, Seattle
		(January 2012).</p>
	<p>"Sounding it Out: Modeling Aurality for Large-scale Text Collection
		Analysis." Modern Language Association (MLA) Conference, Seattle
		(January 2012).</p>
	<p>Clement, T. "Sounding it Out: Modeling Aurality for Large-scale Text
		Collection Analysis." New York Public Library (November 2011).</p>
	<p>"Sounding it out: modeling orality for large-scale text collection
		analysis." Representing Knowledge Conference, University of Kansas
		(September 2011).</p>

	<p>
		<strong>Other</strong>
	</p>
	<p>
		Most Pixels Ever: Cluster Edition at the Texas Advanced Computing
		Center: <a
			href="http://www.tacc.utexas.edu/tacc-projects/a-thousand-words">http://www.tacc.utexas.edu/tacc-projects/a-thousand-words</a>.
	</p>

	<p>
		<strong>Funded by The Andrew W. Mellon Foundation.</strong>
	</p>
</div>

